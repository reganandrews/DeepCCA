{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Regan Andrews - Deep Correlation Learning for Urban Air Quality.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reganandrews/DeepCCA/blob/master/Regan_Andrews_Deep_Correlation_Learning_for_Urban_Air_Quality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pa49bUnKyRgF"
      },
      "source": [
        "# Deep Correlation Learning for Urban Air Quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GU8C5qm_4vZb"
      },
      "source": [
        "This notebook demonstrates the workings of an architecture for deep correlation learning for urban air quality. Submitted in partial fulfilment for the degree of Doctor of Computing by Regan Andrews.\n",
        "\n",
        "Firstly, the environment is setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7rZnJaGTWQw0",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "import seaborn as sns\n",
        "from string import ascii_letters\n",
        "import IPython\n",
        "\n",
        "import pickle\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TokBlnUhWFw9"
      },
      "source": [
        "## Importing the Dataset\n",
        "This architecture uses a dataset created from data provided by the Auckland Council. It contains 9 different features such as atmospheric chemistry, air temperature, solar radiation and humidity. These were collected hourly, beginning in 2012.\n",
        "\n",
        "All data from this dataset can be re-collected from the Auckland Council's Environment Auckland website: https://environmentauckland.org.nz/\n",
        "\n",
        "Locations are indicated in this map: (link here)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xyv_i85IWInT",
        "colab": {}
      },
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://github.com/reganandrews/airdata/raw/master/site-gleneden.csv.zip',\n",
        "    fname='site-gleneden.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TX6uGeeeWIkG",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(csv_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VdbOWXiTWM2T"
      },
      "source": [
        "On viewing the imported data we see it has the following types of data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ojHE-iCCWIhz",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KICd5Yz4zgrZ",
        "colab_type": "text"
      },
      "source": [
        "### Exploration of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8j0hzVpzuKk",
        "colab_type": "text"
      },
      "source": [
        "We are interested in the correlative aspects of this data. Let's have a look and see what commonly accepted correlative measurements uncover."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2X6JLP_z6JD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr_matrix = df.corr()\n",
        "print(corr_matrix)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8e8bXQvDGqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr = df.corr()\n",
        "ax = sns.heatmap(\n",
        "    corr, \n",
        "    vmin=-1, vmax=1, center=0,\n",
        "    cmap=sns.diverging_palette(20, 220, n=200),\n",
        "    square=True\n",
        ")\n",
        "ax.set_xticklabels(\n",
        "    ax.get_xticklabels(),\n",
        "    rotation=45,\n",
        "    horizontalalignment='right'\n",
        ");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCAWbvmoDmX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set(style=\"white\")\n",
        "\n",
        "corr = df.corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.zeros_like(corr, dtype=np.bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_nmCGSVEA6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = plt.figure(figsize=(19, 15))\n",
        "plt.matshow(df.corr(), fignum=f.number)\n",
        "plt.xticks(range(df.shape[1]), df.columns, fontsize=14, rotation=45)\n",
        "plt.yticks(range(df.shape[1]), df.columns, fontsize=14)\n",
        "cb = plt.colorbar()\n",
        "cb.ax.tick_params(labelsize=14)\n",
        "plt.title('Correlation Matrix', fontsize=16);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oAmsUI7EIbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#rs = np.random.RandomState(0)\n",
        "#df = pd.DataFrame(rs.rand(10, 10))\n",
        "#corr = df.corr()\n",
        "#corr.style.background_gradient(cmap='coolwarm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvvKXR4PEWNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.hist()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qfbpcV0MWQzl"
      },
      "source": [
        "As you can see above, an observation is recorded every hour. This means that, for a single hour, you will have 9 observations. Similarly, a single day will contain 24 (9x24) observations. \n",
        "\n",
        "Given a specific time, let's assume we want to predict the temperature 6 hours in the future. In order to make this prediction, you choose to use 5 days of observations. Thus, you would create a window containing the last 720(5x144) observations to train the model. Many such configurations are possible, making this dataset a good one to experiment with.\n",
        "\n",
        "The function below returns the above described windows of time for the model to train on. The parameter `history_size` is the size of the past window of information. The `target_size` is how far in the future does the model need to learn to predict. The `target_size` is the label that needs to be predicted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7AoxQuTrWIbi",
        "colab": {}
      },
      "source": [
        "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
        "  data = []\n",
        "  labels = []\n",
        "\n",
        "  start_index = start_index + history_size\n",
        "  if end_index is None:\n",
        "    end_index = len(dataset) - target_size\n",
        "\n",
        "  for i in range(start_index, end_index):\n",
        "    indices = range(i-history_size, i)\n",
        "    # Reshape data from (history_size,) to (history_size, 1)\n",
        "    data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
        "    labels.append(dataset[i+target_size])\n",
        "  return np.array(data), np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qoFJZmXBaxCc"
      },
      "source": [
        "In all of the following the first 100,000 rows of the data will be the training dataset, and the remaining will be the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ia-MPAHxbInX",
        "colab": {}
      },
      "source": [
        "TRAIN_SPLIT = 100000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EowWDtaNnH1y"
      },
      "source": [
        "We will set the seed to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-x-GgENynHdx",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8YEwr-NoWUpV"
      },
      "source": [
        "## 1) Forecasting a univariate time-series (Temperature) for a single location (Glen Eden)\n",
        "First, you will train a model using only a single feature (temperature), and use it to make predictions for that value in the future.\n",
        "\n",
        "Let's first extract only the temperature from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nbdcnm1_WIY9",
        "colab": {}
      },
      "source": [
        "uni_data = df['gleneden.temp']\n",
        "uni_data.index = df['DateTime']\n",
        "uni_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aQB-46MyWZMm"
      },
      "source": [
        "Let's observe how this data looks across time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ftOExwAqWXSU",
        "colab": {}
      },
      "source": [
        "uni_data.plot(subplots=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ejSEiDqBWXQa",
        "colab": {}
      },
      "source": [
        "uni_data = uni_data.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-eFckdUUHWmT"
      },
      "source": [
        "It is important to normalize features before training a neural network. A common way to do so is by subtracting the mean and dividing by the standard deviation of each feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mxbIic5TMlxx"
      },
      "source": [
        "Note: The mean and standard deviation should only be computed using the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Eji6njXvHusN",
        "colab": {}
      },
      "source": [
        "uni_train_mean = uni_data[:TRAIN_SPLIT].mean()\n",
        "uni_train_std = uni_data[:TRAIN_SPLIT].std()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8Gob1YJYH0cH"
      },
      "source": [
        "Let's normalize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BO55yRD6H0Dx",
        "colab": {}
      },
      "source": [
        "uni_data = (uni_data-uni_train_mean)/uni_train_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gn8A_nrccKtn"
      },
      "source": [
        "Let's now create the data for the univariate model. For part 1, the model will be given the last 20 recorded temperature observations, and needs to learn to predict the temperature at the next time step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aJJ-T49vWXOZ",
        "colab": {}
      },
      "source": [
        "univariate_past_history = 20\n",
        "univariate_future_target = 0\n",
        "\n",
        "x_train_uni, y_train_uni = univariate_data(uni_data, 0, TRAIN_SPLIT,\n",
        "                                           20,\n",
        "                                           0)\n",
        "x_val_uni, y_val_uni = univariate_data(uni_data, TRAIN_SPLIT, None,\n",
        "                                       20,\n",
        "                                       0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aWpVMENsdp0N"
      },
      "source": [
        "This is what the `univariate_data` function returns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "feDd95XFdz5H",
        "colab": {}
      },
      "source": [
        "print ('Single window of past history')\n",
        "print (x_train_uni[0])\n",
        "print ('\\n Target temperature to predict')\n",
        "print (y_train_uni[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hni3Jt9OMR1_"
      },
      "source": [
        "Now that the data has been created, let's take a look at a single example. The information given to the network is given in blue, and it must predict the value at the red cross."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qVukM9dRipop",
        "colab": {}
      },
      "source": [
        "def create_time_steps(length):\n",
        "  time_steps = []\n",
        "  for i in range(-length, 0, 1):\n",
        "    time_steps.append(i)\n",
        "  return time_steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QQeGvh7cWXMR",
        "colab": {}
      },
      "source": [
        "def show_plot(plot_data, delta, title):\n",
        "  labels = ['History', 'True Future', 'Model Prediction']\n",
        "  marker = ['.-', 'rx', 'go']\n",
        "  time_steps = create_time_steps(plot_data[0].shape[0])\n",
        "  if delta:\n",
        "    future = delta\n",
        "  else:\n",
        "    future = 0\n",
        "\n",
        "  plt.title(title)\n",
        "  for i, x in enumerate(plot_data):\n",
        "    if i:\n",
        "      plt.plot(future, plot_data[i], marker[i], markersize=10,\n",
        "               label=labels[i])\n",
        "    else:\n",
        "      plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
        "  plt.legend()\n",
        "  plt.xlim([time_steps[0], (future+5)*2])\n",
        "  plt.xlabel('Time-Step')\n",
        "  return plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pd05iV-UWXKL",
        "colab": {}
      },
      "source": [
        "show_plot([x_train_uni[0], y_train_uni[0]], 0, 'Sample Example')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b5rUJ_2YMWzG"
      },
      "source": [
        "### Baseline\n",
        "Before proceeding to train a model, let's first set a simple baseline. Given an input point, the baseline method looks at all the history and predicts the next point to be the average of the last 20 observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P9nYWcxMMWnr",
        "colab": {}
      },
      "source": [
        "def baseline(history):\n",
        "  return np.mean(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KMcdFYKQMWlm",
        "colab": {}
      },
      "source": [
        "show_plot([x_train_uni[0], y_train_uni[0], baseline(x_train_uni[0])], 0,\n",
        "           'Baseline Prediction Example')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "067m6t8cMakb"
      },
      "source": [
        "Let's see if you can beat this baseline using a recurrent neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H4crpOcoMlSe"
      },
      "source": [
        "### Recurrent neural network\n",
        "\n",
        "A Recurrent Neural Network (RNN) is a type of neural network well-suited to time series data. RNNs process a time series step-by-step, maintaining an internal state summarizing the information they've seen so far. For more details, read the [RNN tutorial](https://www.tensorflow.org/tutorials/sequences/recurrent). In this tutorial, you will use a specialized RNN layer called Long Short Term Memory ([LSTM](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM))\n",
        "\n",
        "Let's now use `tf.data` to shuffle, batch, and cache the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kk-evkrmMWh9",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "train_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))\n",
        "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "val_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))\n",
        "val_univariate = val_univariate.batch(BATCH_SIZE).repeat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n2AmKkyVS5Ht"
      },
      "source": [
        "The following visualisation should help you understand how the data is represented after batching.\n",
        "\n",
        "![Time Series](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/time_series.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4nagdTRNfPuZ"
      },
      "source": [
        "You will see the LSTM requires the input shape of the data it is being given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IDbpHosCMWZO",
        "colab": {}
      },
      "source": [
        "simple_lstm_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(8, input_shape=x_train_uni.shape[-2:]),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "simple_lstm_model.compile(optimizer='adam', loss='mae')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NOGZtDAqMtSi"
      },
      "source": [
        "Let's make a sample prediction, to check the output of the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2mPZbIKCMtLR",
        "colab": {}
      },
      "source": [
        "for x, y in val_univariate.take(1):\n",
        "    print(simple_lstm_model.predict(x).shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QYz6RN_mMyau"
      },
      "source": [
        "Let's train the model now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0opH9xi5MtIk",
        "colab": {}
      },
      "source": [
        "EVALUATION_INTERVAL = 50\n",
        "EPOCHS = 20\n",
        "\n",
        "simple_lstm_model.fit(train_univariate, epochs=EPOCHS,\n",
        "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                      validation_data=val_univariate, validation_steps=50)\n",
        "\n",
        "simple_lstm_model.save(f\"/content/gdrive/My Drive/Colab_Files//Models/simple_lstm_model.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "euyPo_lyNryZ"
      },
      "source": [
        "#### Predict using the simple LSTM model\n",
        "Now the simple LSTM model has been trained, we'll make some predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S2rRLrs8MtGU",
        "colab": {}
      },
      "source": [
        "for x, y in val_univariate.take(3):\n",
        "  plot = show_plot([x[0].numpy(), y[0].numpy(),\n",
        "                    simple_lstm_model.predict(x)[0]], 0, 'Simple LSTM model')\n",
        "  plot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q-AVEJyRNvt0"
      },
      "source": [
        "This looks better than the baseline. We will now work with a multivariate time series apporach for this single location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VlJYi3_HXcw8"
      },
      "source": [
        "## 2) Forecasting a multivariate time-series for a single location (Glen Eden)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hoxNZ2GM7DPm"
      },
      "source": [
        "The original dataset contains 9 features which we'll use now to forecast at a single site - Glen Eden. The features to be used are:\n",
        "\n",
        "\n",
        "1. NO\n",
        "2. NO2\n",
        "3. NOx\n",
        "4. PM10\n",
        "5. Humidity\n",
        "6. Solar Radiation\n",
        "7. Temperature\n",
        "8. Wind\n",
        "9. Wind Speed\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DphrB7bxSNDd",
        "colab": {}
      },
      "source": [
        "features_considered = ['gleneden.no', 'gleneden.no2', 'gleneden.nox', 'geneden.pm10',\t'gleneden.hum',\t'gleneden.solar',\t'gleneden.temp',\t'gleneden.wind',\t'gleneden.ws']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IfQUSiJfUpXJ",
        "colab": {}
      },
      "source": [
        "features = df[features_considered]\n",
        "features.index = df['DateTime']\n",
        "features.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qSfhTZi5r15R"
      },
      "source": [
        "Let's have a look at how each of these features vary across time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QdgC8zvGr21X",
        "colab": {}
      },
      "source": [
        "features.plot(subplots=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cqStgZ-O1b3_"
      },
      "source": [
        "As mentioned, the first step will be to normalize the dataset using the mean and standard deviation of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W7VuNIwfHRHx",
        "colab": {}
      },
      "source": [
        "dataset = features.values\n",
        "data_mean = dataset.mean(axis=0)\n",
        "data_std = dataset.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eJUeWDqploCt",
        "colab": {}
      },
      "source": [
        "dataset = (dataset-data_mean)/data_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LyuGuJUgjUK3"
      },
      "source": [
        "### Single step model\n",
        "In a single step setup, the model learns to predict a single point in the future based on some history provided.\n",
        "\n",
        "The below function performs the same windowing task as below, however, here it samples the past observation based on the step size given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d-rVX4d3OF86",
        "colab": {}
      },
      "source": [
        "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
        "                      target_size, step, single_step=False):\n",
        "  data = []\n",
        "  labels = []\n",
        "\n",
        "  start_index = start_index + history_size\n",
        "  if end_index is None:\n",
        "    end_index = len(dataset) - target_size\n",
        "\n",
        "  for i in range(start_index, end_index):\n",
        "    indices = range(i-history_size, i, step)\n",
        "    data.append(dataset[indices])\n",
        "\n",
        "    if single_step:\n",
        "      labels.append(target[i+target_size])\n",
        "    else:\n",
        "      labels.append(target[i:i+target_size])\n",
        "\n",
        "  return np.array(data), np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HWVGYwbN2ITI"
      },
      "source": [
        "The network is shown data from the last 720 observations that are sampled hourly. The sampling is done every one hour since a drastic change is not expected within 60 minutes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HlhVGzPhmMYI",
        "colab": {}
      },
      "source": [
        "past_history = 720\n",
        "future_target = 72\n",
        "STEP = 6\n",
        "\n",
        "x_train_single, y_train_single = multivariate_data(dataset, dataset[:, 1], 0,\n",
        "                                                   TRAIN_SPLIT, past_history,\n",
        "                                                   future_target, STEP,\n",
        "                                                   single_step=True)\n",
        "x_val_single, y_val_single = multivariate_data(dataset, dataset[:, 1],\n",
        "                                               TRAIN_SPLIT, None, past_history,\n",
        "                                               future_target, STEP,\n",
        "                                               single_step=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CamMObrwPhnp"
      },
      "source": [
        "Let's look at a single data-point.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_tVKm-ZIPls0",
        "colab": {}
      },
      "source": [
        "print ('Single window of past history : {}'.format(x_train_single[0].shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eCWG4xgQ3O6E",
        "colab": {}
      },
      "source": [
        "train_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))\n",
        "train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
        "val_data_single = val_data_single.batch(BATCH_SIZE).repeat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0aWec9_nlxBl",
        "colab": {}
      },
      "source": [
        "single_step_model = tf.keras.models.Sequential()\n",
        "single_step_model.add(tf.keras.layers.LSTM(32,\n",
        "                                           input_shape=x_train_single.shape[-2:]))\n",
        "single_step_model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "single_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oYhUfWjwOPFN"
      },
      "source": [
        "Let's check out a sample prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yY7FodHVOPsH",
        "colab": {}
      },
      "source": [
        "for x, y in val_data_single.take(1):\n",
        "  print(single_step_model.predict(x).shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U0jnt2l2mwkl",
        "colab": {}
      },
      "source": [
        "single_step_history = single_step_model.fit(train_data_single, epochs=EPOCHS,\n",
        "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                                            validation_data=val_data_single,\n",
        "                                            validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-ZAdeAnP5c72",
        "colab": {}
      },
      "source": [
        "def plot_train_history(history, title):\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(loss))\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "  plt.title(title)\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l8lBKA-z5yYV",
        "colab": {}
      },
      "source": [
        "plot_train_history(single_step_history,\n",
        "                   'Single Step Training and validation loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DfjrGAlEUp7i"
      },
      "source": [
        "#### Predict a single step future\n",
        "Now that the model is trained, let's make a few sample predictions. The model is given the history of three features over the past five days sampled every hour (120 data-points), since the goal is to predict the temperature, the plot only displays the past temperature. The prediction is made one day into the future (hence the gap between the history and prediction). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h1qmPLLVUpuN",
        "colab": {}
      },
      "source": [
        "for x, y in val_data_single.take(3):\n",
        "  plot = show_plot([x[0][:, 1].numpy(), y[0].numpy(),\n",
        "                    single_step_model.predict(x)[0]], 12,\n",
        "                   'Single Step Prediction')\n",
        "  plot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2GnE087bJYSu"
      },
      "source": [
        "### Multi-Step model\n",
        "In a multi-step prediction model, given a past history, the model needs to learn to predict a range of future values. Thus, unlike a single step model, where only a single future point is predicted, a multi-step model predict a sequence of the future.\n",
        "\n",
        "For the multi-step model, the training data again consists of recordings over the past five days sampled every hour. However, here, the model needs to learn to predict the temperature for the next 12 hours. Since an obversation is taken every 10 minutes, the output is 72 predictions. For this task, the dataset needs to be prepared accordingly, thus the first step is just to create it again, but with a different target window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kZCk9fqyJZqX",
        "colab": {}
      },
      "source": [
        "future_target = 72\n",
        "x_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 1], 0,\n",
        "                                                 TRAIN_SPLIT, past_history,\n",
        "                                                 future_target, STEP)\n",
        "x_val_multi, y_val_multi = multivariate_data(dataset, dataset[:, 1],\n",
        "                                             TRAIN_SPLIT, None, past_history,\n",
        "                                             future_target, STEP)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LImXPwAGRtWy"
      },
      "source": [
        "Let's check out a sample data-point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SpWDcBkQRwS-",
        "colab": {}
      },
      "source": [
        "print ('Single window of past history : {}'.format(x_train_multi[0].shape))\n",
        "print ('\\n Target temperature to predict : {}'.format(y_train_multi[0].shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cjR4PJArMOpA",
        "colab": {}
      },
      "source": [
        "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
        "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
        "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IZcg8FWpSG8K"
      },
      "source": [
        "Plotting a sample data-point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ksXKVbwBV7D3",
        "colab": {}
      },
      "source": [
        "def multi_step_plot(history, true_future, prediction):\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  num_in = create_time_steps(len(history))\n",
        "  num_out = len(true_future)\n",
        "\n",
        "  plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
        "  plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
        "           label='True Future')\n",
        "  if prediction.any():\n",
        "    plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
        "             label='Predicted Future')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LCQKetflZRMF"
      },
      "source": [
        "In this plot and subsequent similar plots, the history and the future data are sampled every hour."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R6G8bacQR4w2",
        "colab": {}
      },
      "source": [
        "for x, y in train_data_multi.take(1):\n",
        "  multi_step_plot(x[0], y[0], np.array([0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XOjz8DzZ4HFS"
      },
      "source": [
        "Since the task here is a bit more complicated than the previous task, the model now consists of two LSTM layers. Finally, since 72 predictions are made, the dense layer outputs 72 predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "byAl0NKSNBP6",
        "colab": {}
      },
      "source": [
        "multi_step_model = tf.keras.models.Sequential()\n",
        "multi_step_model.add(tf.keras.layers.LSTM(32,\n",
        "                                          return_sequences=True,\n",
        "                                          input_shape=x_train_multi.shape[-2:]))\n",
        "multi_step_model.add(tf.keras.layers.LSTM(16, activation='relu'))\n",
        "multi_step_model.add(tf.keras.layers.Dense(72))\n",
        "\n",
        "multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UvB7zBqVSMyl"
      },
      "source": [
        "Let's see how the model predicts before it trains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "13_ZWvB9SRlZ",
        "colab": {}
      },
      "source": [
        "for x, y in val_data_multi.take(1):\n",
        "  print (multi_step_model.predict(x).shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7uwOhXo3Oems",
        "colab": {}
      },
      "source": [
        "multi_step_history = multi_step_model.fit(train_data_multi, epochs=EPOCHS,\n",
        "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                                          validation_data=val_data_multi,\n",
        "                                          validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UKfQoBjQ5l7U",
        "colab": {}
      },
      "source": [
        "plot_train_history(multi_step_history, 'Multi-Step Training and validation loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oDg94-yq4pas"
      },
      "source": [
        "#### Predict a multi-step future\n",
        "Let's now have a look at how well your network has learnt to predict the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dt22wq6fyIBU",
        "colab": {}
      },
      "source": [
        "for x, y in val_data_multi.take(3):\n",
        "  multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX27h0MzmMT6",
        "colab_type": "text"
      },
      "source": [
        "## 3) Which locations have a correlation?\n",
        "\n",
        "The original dataset contains x locations, with a total of 109 features.\n",
        "\n",
        "If we can use this dataset to determine which locations are closely correlated to 1 target location, we should be able to more accurately predict the results at that target location by the relevent data.\n",
        "\n",
        "We'll start by looking at whether the chemical 'NO' and seeing what locations show correlation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWQyJoTVmyxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://github.com/reganandrews/airdata/raw/master/NO.csv.zip',\n",
        "    fname='NO.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRcSIwapm3DM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(csv_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlWCJV-fnPfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcIZRpnEnfCy",
        "colab_type": "text"
      },
      "source": [
        "The correlation matrix for the master data shows the correlations that exist between all locations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLahfHgbneEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr_matrix = df.corr()\n",
        "print(corr_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3tWxOzs81Px",
        "colab_type": "text"
      },
      "source": [
        "What about another parameter, are the correlation values still similar? Let's try NO2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azGjQsQ686D7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://github.com/reganandrews/airdata/raw/master/NO2.csv.zip',\n",
        "    fname='NO2.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImrJmU479AGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(csv_path,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ9zz9hU9Gqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzvWzCES9JBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr_matrix = df.corr()\n",
        "print(corr_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pOzaIRYBhqwg"
      },
      "source": [
        "## 4) Forecasting a multi-variate time-series for one location using the surrounding locations data\n",
        "\n",
        "The original dataset contains x locations, with a total of 109 features which we'll use now to forecast everything at once.\n",
        "\n",
        "We'll import this new data now from GitHub:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj_vQe-M946l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://github.com/reganandrews/airdata/raw/master/master_data_fixed.csv.zip',\n",
        "    fname='master_data_fixed.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_rPu-O89_Hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(csv_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_VPTnqy9_zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPCiU1mjwLTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_considered = ['gleneden.co', 'henderson.co', 'khyberpass.co', 'pakuranga.co', 'poal.co', 'queenst.co', 'takapuna.co', 'gleneden.no', 'gleneden.no2','gleneden.nox', 'henderson.no', 'henderson.no2', 'henderson.nox', 'musickpt.no', 'musickpt.no2', 'musickpt.nox', 'patumahoe.no', 'patumahoe.no2', 'patumahoe.nox', 'penrose.no', 'penrose.no2', 'penrose.nox', 'poal.no', 'poal.no2', 'poal.nox', 'queenst.no', 'queenst.no2', 'queenst.nox', 'takapuna.no', 'takapuna.no2', 'takapuna.nox', 'musickpt.o3', 'patumahoe.o3', 'whangarei.o3', 'botanydowns.pm10', 'geneden.pm10', 'henderson.pm10', 'khyberpass.pm10', 'kumeu.pm10', 'orewa.pm10', 'pakuranga.pm10', 'patumahoe.pm10', 'patumahoe.pm25', 'penrose.pm1.grimm', 'penrose.pm10', 'penrose.pm10.grimm', 'penrose.pm25', 'penrose.pm25.grimm', 'poal.pm10', 'poal.pm25', 'takapuna.pm10', 'takapuna.pm25', 'whangarei.pm10', 'whangaparoa.pm25', 'pakuranga.rain', 'penrose.rain', 'poal.rain', 'takapuna.rain', 'botanydowns.hum', 'gleneden.hum', 'henderson.hum', 'musickpt.hum', 'orewa.hum', 'pakuranga.hum', 'penrose.hum', 'poal.hum', 'takapuna.hum', 'penrose.so2', 'poal.so2', 'gleneden.solar', 'henderson.solar', 'musickpt.solar', 'orewa.solar', 'pakuranga.solar', 'penrose.solar', 'poal.solar', 'takapuna.solar', 'gleneden.temp', 'henderson.temp', 'musickpt.temp', 'orewa.temp', 'pakuranga.temp', 'penrose.temp', 'poal.temp', 'takapuna.temp', 'botanydowns.wind', 'gleneden.wind', 'henderson.wind', 'musickpt.wind', 'orewa.wind', 'pakuranga.wind', 'penrose.wind', 'poal.wind', 'takapuna.wind', 'botanydowns.windspeed', 'gleneden.windspeed', 'henderson.windspeed', 'pakuranga.windspeed', 'penrose.windspeed', 'poal.windspeed', 'takapuna.windspeed']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4Bbx7kiUeO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = df[features_considered]\n",
        "features.index = df['DateTime']\n",
        "features.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc_u_uQwUj-I",
        "colab_type": "text"
      },
      "source": [
        "How do these features vary across time?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrf8zK2vUnWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features.plot(subplots=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV4QRlR1Ur1y",
        "colab_type": "text"
      },
      "source": [
        "As for the previous single location multi-variate time-series, the first step will be to normalize the dataset using the mean and standard deviation of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVN4JRp9UxiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataset = features.values\n",
        "#data_mean = dataset.mean(axis=0)\n",
        "#data_std = dataset.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziUz5T-kU3Us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataset = (dataset-data_mean)/data_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORUWqknLVC6t",
        "colab_type": "text"
      },
      "source": [
        "### Single step model\n",
        "In the single step setup, the model learns to predict a single point in the future based on some history provided.\n",
        "\n",
        "The below function performs the same windowing task as before, however, here it samples the past observation based on the step size given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8sE8LB2VJEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
        "                      target_size, step, single_step=False):\n",
        "  data = []\n",
        "  labels = []\n",
        "\n",
        "  start_index = start_index + history_size\n",
        "  if end_index is None:\n",
        "    end_index = len(dataset) - target_size\n",
        "\n",
        "  for i in range(start_index, end_index):\n",
        "    indices = range(i-history_size, i, step)\n",
        "    data.append(dataset[indices])\n",
        "\n",
        "    if single_step:\n",
        "      labels.append(target[i+target_size])\n",
        "    else:\n",
        "      labels.append(target[i:i+target_size])\n",
        "\n",
        "  return np.array(data), np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b8bD2_VVeir",
        "colab_type": "text"
      },
      "source": [
        "The network is shown data from the last 720 observations that are sampled hourly. The sampling is done every one hour since a drastic change is not expected within 60 minutes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIutYO8OVf9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "past_history = 720\n",
        "future_target = 72\n",
        "STEP = 6\n",
        "\n",
        "x_train_single, y_train_single = multivariate_data(dataset, dataset[:, 1], 0,\n",
        "                                                   TRAIN_SPLIT, past_history,\n",
        "                                                   future_target, STEP,\n",
        "                                                   single_step=True)\n",
        "x_val_single, y_val_single = multivariate_data(dataset, dataset[:, 1],\n",
        "                                               TRAIN_SPLIT, None, past_history,\n",
        "                                               future_target, STEP,\n",
        "                                               single_step=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUTSYhhxVrHj",
        "colab_type": "text"
      },
      "source": [
        "Let's check a single data point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-LHszwQVsrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print ('Single window of past history : {}'.format(x_train_single[0].shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31MzWFk8Vw-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "single_step_history = single_step_model.fit(train_data_single, epochs=EPOCHS,\n",
        "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                                            validation_data=val_data_single,\n",
        "                                            validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhEpMeP_V2yT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_train_history(history, title):\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(loss))\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "  plt.title(title)\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNpE3zvoV8PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_train_history(single_step_history,\n",
        "                   'Single Step Training and validation loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMWvapGoo2cs",
        "colab_type": "text"
      },
      "source": [
        "Code after this point is untested/dev."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCkC05nRo6Qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "#import tensorflow as tf\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import math\n",
        "from keras.optimizers import RMSprop, SGD\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "import seaborn as sns\n",
        "from string import ascii_letters\n",
        "import IPython\n",
        "\n",
        "import pickle\n",
        "\n",
        "from keras.layers import Dense\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2\n",
        "\n",
        "import scipy.io as sio\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPzs0gAg_8GM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://github.com/reganandrews/airdata/raw/master/master_data_fixed.csv.zip',\n",
        "    fname='master_data_fixed.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)\n",
        "\n",
        "f = pd.read_csv(csv_path)\n",
        "\n",
        "print('Loaded Data - Master File')\n",
        "\n",
        "f.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYPBJWKt_9nG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://github.com/reganandrews/airdata/raw/master/location-gleneden.csv.zip',\n",
        "    fname='location-gleneden.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)\n",
        "\n",
        "H1 = pd.read_csv(csv_path)\n",
        "\n",
        "print('Loaded Data - Glen Eden')\n",
        "\n",
        "H1.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qTeVBDrAFDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://github.com/reganandrews/airdata/raw/master/location-henderson.csv.zip',\n",
        "    fname='location-henderson.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)\n",
        "\n",
        "H2 = pd.read_csv(csv_path)\n",
        "\n",
        "print('Loaded Data - Henderson')\n",
        "\n",
        "H2.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ilG0M4YDfvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BEGIN Dataset\n",
        "\n",
        "class DataSet(object):\n",
        "    \n",
        "    def __init__(self, images1, images2, labels, fake_data=False, one_hot=False,\n",
        "                 dtype=tf.float32):\n",
        "        \"\"\"Construct a DataSet.\n",
        "        one_hot arg is used only if fake_data is true.  `dtype` can be either\n",
        "        `uint8` to leave the input as `[0, 255]`, or `float32` to rescale into\n",
        "        `[0, 1]`.\n",
        "        \"\"\"\n",
        "        dtype = tf.as_dtype(dtype).base_dtype\n",
        "        if dtype not in (tf.uint8, tf.float32):\n",
        "            raise TypeError('Invalid image dtype %r, expected uint8 or float32' % dtype)\n",
        "        \n",
        "        if fake_data:\n",
        "            self._num_examples = 10000\n",
        "            self.one_hot = one_hot\n",
        "        else:\n",
        "            assert images1.shape[0] == labels.shape[0], (\n",
        "                'images1.shape: %s labels.shape: %s' % (images1.shape,\n",
        "                                                        labels.shape))\n",
        "            assert images2.shape[0] == labels.shape[0], (\n",
        "                'images2.shape: %s labels.shape: %s' % (images2.shape,\n",
        "                                                        labels.shape))\n",
        "            self._num_examples = images1.shape[0]\n",
        "            # Convert shape from [num examples, rows, columns, depth]\n",
        "            # to [num examples, rows*columns] (assuming depth == 1)\n",
        "            #assert images.shape[3] == 1\n",
        "            #images = images.reshape(images.shape[0],\n",
        "            #                        images.shape[1] * images.shape[2])\n",
        "            if dtype == tf.float32 and images1.dtype != np.float32:\n",
        "                # Convert from [0, 255] -> [0.0, 1.0].\n",
        "                print(\"type conversion view 1\")\n",
        "                images1 = images1.astype(np.float32)\n",
        "            \n",
        "            if dtype == tf.float32 and images2.dtype != np.float32:\n",
        "                print(\"type conversion view 2\")\n",
        "                images2 = images2.astype(np.float32)\n",
        "\n",
        "        self._images1 = images1\n",
        "        self._images2 = images2\n",
        "        self._labels = labels\n",
        "        self._epochs_completed = 0\n",
        "        self._index_in_epoch = 0\n",
        "    \n",
        "    @property\n",
        "    def images1(self):\n",
        "        return self._images1\n",
        "    \n",
        "    @property\n",
        "    def images2(self):\n",
        "        return self._images2\n",
        "    \n",
        "    @property\n",
        "    def labels(self):\n",
        "        return self._labels\n",
        "    \n",
        "    @property\n",
        "    def num_examples(self):\n",
        "        return self._num_examples\n",
        "    \n",
        "    @property\n",
        "    def epochs_completed(self):\n",
        "        return self._epochs_completed\n",
        "    \n",
        "    def next_batch(self, batch_size, fake_data=False):\n",
        "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
        "        if fake_data:\n",
        "            fake_image = [1] * 784\n",
        "            if self.one_hot:\n",
        "                fake_label = [1] + [0] * 9\n",
        "            else:\n",
        "                fake_label = 0\n",
        "            return [fake_image for _ in xrange(batch_size)], [fake_image for _ in xrange(batch_size)], [fake_label for _ in xrange(batch_size)]\n",
        "        \n",
        "        start = self._index_in_epoch\n",
        "        self._index_in_epoch += batch_size\n",
        "        if self._index_in_epoch > self._num_examples:\n",
        "            # Finished epoch\n",
        "            self._epochs_completed += 1\n",
        "            # Shuffle the data\n",
        "            perm = np.arange(self._num_examples)\n",
        "            np.random.shuffle(perm)\n",
        "            self._images1 = self._images1[perm]\n",
        "            self._images2 = self._images2[perm]\n",
        "            self._labels = self._labels[perm]\n",
        "            # Start next epoch\n",
        "            start = 0\n",
        "            self._index_in_epoch = batch_size\n",
        "            assert batch_size <= self._num_examples\n",
        "        \n",
        "        end = self._index_in_epoch\n",
        "        return self._images1[start:end], self._images2[start:end], self._labels[start:end]\n",
        "\n",
        "def read_mnist():\n",
        "\n",
        "    data=sio.loadmat('MNIST.mat')\n",
        "    \n",
        "    train=DataSet(data['X1'],data['X2'],data['trainLabel'])\n",
        "    \n",
        "    tune=DataSet(data['XV1'],data['XV2'],data['tuneLabel'])\n",
        "    \n",
        "    test=DataSet(data['XTe1'],data['XTe2'],data['testLabel'])\n",
        "    \n",
        "    return train, tune, test\n",
        "\n",
        "\n",
        "def read_xrmb():\n",
        "\n",
        "    data=sio.loadmat('/share/data/speech-multiview/wwang5/cca/XRMBf2KALDI_window7_single.mat')\n",
        "    \n",
        "    train=DataSet(data['X1'],data['X2'],data['trainLabel'])\n",
        "    \n",
        "    tune=DataSet(data['XV1'],data['XV2'],data['tuneLabel'])\n",
        "    \n",
        "    test=DataSet(data['XTe1'],data['XTe2'],data['testLabel'])\n",
        "    \n",
        "    return train, tune, test\n",
        "\n",
        "    \n",
        "def read_flicker():\n",
        "\n",
        "    data=sio.loadmat('/share/data/speech-multiview/wwang5/cca/VCCA/flicker/flicker_tensorflow_split1.mat')\n",
        "    X1=data['X1']\n",
        "    X2=data['X2']\n",
        "    XV1=data['XV1']\n",
        "    XV2=data['XV2']\n",
        "    XTe1=data['XTe1']\n",
        "    XTe2=data['XTe2']\n",
        "    \n",
        "    for i in range(2,11):\n",
        "        \n",
        "        data=sio.loadmat('/share/data/speech-multiview/wwang5/cca/VCCA/flicker/flicker_tensorflow_split' + str(i) + '.mat')\n",
        "        \n",
        "        X1=np.concatenate([X1, data['X1']])\n",
        "        X2=np.concatenate([X2, data['X2']])\n",
        "        XV1=np.concatenate([XV1, data['XV1']])\n",
        "        XV2=np.concatenate([XV2, data['XV2']])\n",
        "        XTe1=np.concatenate([XTe1, data['XTe1']])\n",
        "        XTe2=np.concatenate([XTe2, data['XTe2']])\n",
        "    \n",
        "    train=DataSet(X1, X2, np.zeros(len(X1)))\n",
        "    \n",
        "    tune=DataSet(XV1, XV2, np.zeros(len(XV1)))\n",
        "    \n",
        "    test=DataSet(XTe1, XTe2, np.zeros(len(XTe1)))\n",
        "    \n",
        "    return train, tune, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pgBjPhM-Y1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BEGIN linear_cca.py\n",
        "def linear_cca(H1, H2, outdim_size):\n",
        "    \"\"\"\n",
        "    An implementation of linear CCA\n",
        "    # Arguments:\n",
        "        H1 and H2: the matrices containing the data for view 1 and view 2. Each row is a sample.\n",
        "        outdim_size: specifies the number of new features\n",
        "    # Returns\n",
        "        A and B: the linear transformation matrices \n",
        "        mean1 and mean2: the means of data for both views\n",
        "    \"\"\"\n",
        "    r1 = 1e-4\n",
        "    r2 = 1e-4\n",
        "\n",
        "    m = H1.shape[0]\n",
        "    o = H1.shape[1]\n",
        "\n",
        "    mean1 = numpy.mean(H1, axis=0)\n",
        "    mean2 = numpy.mean(H2, axis=0)\n",
        "    H1bar = H1 - numpy.tile(mean1, (m, 1))\n",
        "    H2bar = H2 - numpy.tile(mean2, (m, 1))\n",
        "\n",
        "    SigmaHat12 = (1.0 / (m - 1)) * numpy.dot(H1bar.T, H2bar)\n",
        "    SigmaHat11 = (1.0 / (m - 1)) * numpy.dot(H1bar.T, H1bar) + r1 * numpy.identity(o)\n",
        "    SigmaHat22 = (1.0 / (m - 1)) * numpy.dot(H2bar.T, H2bar) + r2 * numpy.identity(o)\n",
        "\n",
        "    [D1, V1] = numpy.linalg.eigh(SigmaHat11)\n",
        "    [D2, V2] = numpy.linalg.eigh(SigmaHat22)\n",
        "    SigmaHat11RootInv = numpy.dot(numpy.dot(V1, numpy.diag(D1 ** -0.5)), V1.T)\n",
        "    SigmaHat22RootInv = numpy.dot(numpy.dot(V2, numpy.diag(D2 ** -0.5)), V2.T)\n",
        "\n",
        "    Tval = numpy.dot(numpy.dot(SigmaHat11RootInv, SigmaHat12), SigmaHat22RootInv)\n",
        "\n",
        "    [U, D, V] = numpy.linalg.svd(Tval)\n",
        "    V = V.T\n",
        "    A = numpy.dot(SigmaHat11RootInv, U[:, 0:outdim_size])\n",
        "    B = numpy.dot(SigmaHat22RootInv, V[:, 0:outdim_size])\n",
        "    D = D[0:outdim_size]\n",
        "\n",
        "    return A, B, mean1, mean2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPuyKCKZ_WYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BEGIN model.py\n",
        "\n",
        "# BEGIN model.py\n",
        "\n",
        "def my_init_sigmoid(shape, dtype=None):\n",
        "    rnd = K.random_uniform(\n",
        "        shape, 0., 1., dtype)\n",
        "    from keras.initializers import _compute_fans\n",
        "    fan_in, fan_out = _compute_fans(shape)\n",
        "    return 8. * (rnd - 0.5) * math.sqrt(6) / math.sqrt(fan_in + fan_out)\n",
        "\n",
        "def my_init_others(shape, dtype=None):\n",
        "    rnd = K.random_uniform(\n",
        "        shape, 0., 1., dtype)\n",
        "    from keras.initializers import _compute_fans\n",
        "    fan_in, fan_out = _compute_fans(shape)\n",
        "    return 2. * (rnd - 0.5) / math.sqrt(fan_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH0x9zt9_iWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeepCCA():\n",
        "    def __init__(self, layer_sizes1,\n",
        "                 layer_sizes2, input_size1,\n",
        "                 input_size2, outdim_size, reg_par, use_all_singular_values):\n",
        "\n",
        "        self.layer_sizes1 = layer_sizes1  # [1024, 1024, 1024, outdim_size]\n",
        "        self.layer_sizes2 = layer_sizes2\n",
        "        self.input_size1 = input_size1\n",
        "        self.input_size2 = input_size2\n",
        "        self.outdim_size = outdim_size\n",
        "\n",
        "        self.input_view1 = tf.placeholder(tf.float32, [None, input_size1])\n",
        "        self.input_view2 = tf.placeholder(tf.float32, [None, input_size2])\n",
        "\n",
        "        self.output_view1 = self.build_mlp_net(self.input_view1, layer_sizes1, reg_par)\n",
        "        self.output_view2 = self.build_mlp_net(self.input_view2, layer_sizes2, reg_par)\n",
        "\n",
        "        self.neg_corr = self.neg_correlation(self.output_view1, self.output_view2, use_all_singular_values)\n",
        "\n",
        "    def build_mlp_net(self, input, layer_sizes, reg_par):\n",
        "        output = input\n",
        "        for l_id, ls in enumerate(layer_sizes):\n",
        "            if l_id == len(layer_sizes) - 1:\n",
        "                activation = None\n",
        "                kernel_initializer = my_init_others\n",
        "            else:\n",
        "                activation = tf.nn.sigmoid\n",
        "                kernel_initializer = my_init_sigmoid\n",
        "\n",
        "            output = Dense(ls, activation=activation,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=l2(reg_par))(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def neg_correlation(self, output1, output2, use_all_singular_values):\n",
        "        r1 = 1e-4\n",
        "        r2 = 1e-4\n",
        "        eps = 1e-12\n",
        "\n",
        "        # unpack (separate) the output of networks for view 1 and view 2\n",
        "        H1 = tf.transpose(output1)\n",
        "        H2 = tf.transpose(output2)\n",
        "\n",
        "        m = tf.shape(H1)[1]\n",
        "\n",
        "        H1bar = H1 - (1.0 / tf.cast(m, tf.float32)) * tf.matmul(H1, tf.ones([m, m]))\n",
        "        H2bar = H2 - (1.0 / tf.cast(m, tf.float32)) * tf.matmul(H2, tf.ones([m, m]))\n",
        "\n",
        "        SigmaHat12 = (1.0 / (tf.cast(m, tf.float32) - 1)) * tf.matmul(H1bar, tf.transpose(H2bar))\n",
        "        SigmaHat11 = (1.0 / (tf.cast(m, tf.float32) - 1)) * tf.matmul(H1bar, tf.transpose(H1bar)) + r1 * tf.eye(self.outdim_size)\n",
        "        SigmaHat22 = (1.0 / (tf.cast(m, tf.float32) - 1)) * tf.matmul(H2bar, tf.transpose(H2bar)) + r2 * tf.eye(self.outdim_size)\n",
        "\n",
        "        # Calculating the root inverse of covariance matrices by using eigen decomposition\n",
        "        [D1, V1] = tf.linalg.eigh(SigmaHat11)\n",
        "        [D2, V2] = tf.linalg.eigh(SigmaHat22)\n",
        "\n",
        "        # Added to increase stability\n",
        "        posInd1 = tf.where(tf.greater(D1, eps))\n",
        "        posInd1 = tf.reshape(posInd1, [-1, tf.shape(posInd1)[0]])[0]\n",
        "        D1 = tf.gather(D1, posInd1)\n",
        "        V1 = tf.gather(V1, posInd1)\n",
        "\n",
        "        posInd2 = tf.where(tf.greater(D2, eps))\n",
        "        posInd2 = tf.reshape(posInd2, [-1, tf.shape(posInd2)[0]])[0]\n",
        "        D2 = tf.gather(D2, posInd2)\n",
        "        V2 = tf.gather(V2, posInd2)\n",
        "\n",
        "        SigmaHat11RootInv = tf.matmul(tf.matmul(V1, tf.linalg.diag(D1 ** -0.5)), tf.transpose(V1))\n",
        "        SigmaHat22RootInv = tf.matmul(tf.matmul(V2, tf.linalg.diag(D2 ** -0.5)), tf.transpose(V2))\n",
        "\n",
        "        Tval = tf.matmul(tf.matmul(SigmaHat11RootInv, SigmaHat12), SigmaHat22RootInv)\n",
        "\n",
        "        if use_all_singular_values:\n",
        "            # all singular values are used to calculate the correlation\n",
        "            # corr = tf.sqrt(tf.linalg.trace(tf.matmul(tf.transpose(Tval), Tval)))  ### The usage of \"sqrt\" here is wrong!!!\n",
        "            Tval.set_shape([self.outdim_size, self.outdim_size])\n",
        "            s = tf.svd(Tval, compute_uv=False)\n",
        "            corr = tf.reduce_sum(s)\n",
        "        else:\n",
        "            # just the top outdim_size singular values are used\n",
        "            [U, V] = tf.linalg.eigh(tf.matmul(tf.transpose(Tval), Tval))\n",
        "            non_critical_indexes = tf.where(tf.greater(U, eps))\n",
        "            non_critical_indexes = tf.reshape(non_critical_indexes, [-1, tf.shape(non_critical_indexes)[0]])[0]\n",
        "            U = tf.gather(U, non_critical_indexes)\n",
        "            U = tf.gather(U, tf.nn.top_k(U[:, ]).indices)\n",
        "            corr = tf.reduce_sum(tf.sqrt(U[0:self.outdim_size]))\n",
        "        return -corr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LEJlUh8_j9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BEGIN train.py\n",
        "\n",
        "n_epochs = 100\n",
        "learning_rate = 0.01\n",
        "momentum=0.99\n",
        "batch_size = 800\n",
        "outdim_size = 10\n",
        "input_size1 = 784\n",
        "input_size2 = 784\n",
        "layer_sizes1 = [1024, 1024, 1024, outdim_size]\n",
        "layer_sizes2 = [1024, 1024, 1024, outdim_size]\n",
        "reg_par = 1e-4\n",
        "use_all_singular_values = True\n",
        "\n",
        "\n",
        "trainData, tuneData, testData = read_mnist()\n",
        "\n",
        "dcca_model = DeepCCA(layer_sizes1, layer_sizes2,\n",
        "                      input_size1, input_size2,\n",
        "                      outdim_size,\n",
        "                      reg_par, use_all_singular_values)\n",
        "\n",
        "\n",
        "input_view1 = dcca_model.input_view1\n",
        "input_view2 = dcca_model.input_view2\n",
        "hidden_view1 = dcca_model.output_view1\n",
        "hidden_view2 = dcca_model.output_view2\n",
        "neg_corr = dcca_model.neg_corr\n",
        "\n",
        "\n",
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
        "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
        "\n",
        "train_op = tf.train.MomentumOptimizer(learning_rate, momentum).minimize(neg_corr,\n",
        "                                                                        var_list=tf.trainable_variables())\n",
        "\n",
        "tf.global_variables_initializer().run()\n",
        "\n",
        "\n",
        "iterations = 0\n",
        "for epoch in range(n_epochs):\n",
        "    index = np.arange(trainData.num_examples)\n",
        "    np.random.shuffle(index)\n",
        "    trX1 = trainData.images1[index]\n",
        "    trX2= trainData.images2[index]\n",
        "\n",
        "    for start, end in zip(range(0, trainData.num_examples, batch_size),\n",
        "            range(batch_size, trainData.num_examples, batch_size)):\n",
        "        Xs1 = trX1[start:end]\n",
        "        Xs2 = trX2[start:end]\n",
        "\n",
        "        _, neg_corr_val = sess.run(\n",
        "                                  [train_op, neg_corr],\n",
        "                                   feed_dict={\n",
        "                                       input_view1:Xs1,\n",
        "                                       input_view2:Xs2\n",
        "                                   })\n",
        "\n",
        "\n",
        "        if iterations % 100 == 0:\n",
        "            print(\"iteration:\", iterations)\n",
        "            print(\"neg_loss_for_train:\", neg_corr_val)\n",
        "            tune_neg_corr_val = sess.run(neg_corr,\n",
        "                feed_dict={\n",
        "                    input_view1: tuneData.images1,\n",
        "                    input_view2: tuneData.images2\n",
        "                })\n",
        "            print(\"neg_loss_for_tune:\", tune_neg_corr_val)\n",
        "\n",
        "        iterations += 1\n",
        "\n",
        "\n",
        "################# Linear CCA #############################\n",
        "\n",
        "X1proj, X2proj = sess.run(\n",
        "                        [hidden_view1, hidden_view2],\n",
        "                        feed_dict={\n",
        "                            input_view1: trainData.images1,\n",
        "                            input_view2: trainData.images2\n",
        "                        })\n",
        "XV1proj, XV2proj = sess.run(\n",
        "                        [hidden_view1, hidden_view2],\n",
        "                        feed_dict={\n",
        "                            input_view1: tuneData.images1,\n",
        "                            input_view2: tuneData.images2\n",
        "                        })\n",
        "XTe1proj, XTe2proj = sess.run(\n",
        "                        [hidden_view1, hidden_view2],\n",
        "                        feed_dict={\n",
        "                            input_view1: testData.images1,\n",
        "                            input_view2: testData.images2\n",
        "                        })\n",
        "print(\"Linear CCA started!\")\n",
        "w = [None, None]\n",
        "m = [None, None]\n",
        "w[0], w[1], m[0], m[1] = linear_cca(X1proj, X2proj, 10)\n",
        "print(\"Linear CCA ended!\")\n",
        "X1proj -= m[0].reshape([1, -1]).repeat(len(X1proj), axis=0)\n",
        "X1proj = np.dot(X1proj, w[0])\n",
        "\n",
        "XV1proj -= m[0].reshape([1, -1]).repeat(len(XV1proj), axis=0)\n",
        "XV1proj = np.dot(XV1proj, w[0])\n",
        "\n",
        "XTe1proj -= m[0].reshape([1, -1]).repeat(len(XTe1proj), axis=0)\n",
        "XTe1proj = np.dot(XTe1proj, w[0])\n",
        "\n",
        "trainLable = trainData.labels.astype('float')\n",
        "tuneLable = tuneData.labels.astype('float')\n",
        "testLable = testData.labels.astype('float')\n",
        "\n",
        "\n",
        "################# SVM classify #############################\n",
        "\n",
        "print('training SVM...')\n",
        "clf = svm.LinearSVC(C=0.01, dual=False)\n",
        "clf.fit(X1proj, trainLable.ravel())\n",
        "\n",
        "p = clf.predict(XTe1proj)\n",
        "test_acc = accuracy_score(testLable, p)\n",
        "p = clf.predict(XV1proj)\n",
        "valid_acc = accuracy_score(tuneLable, p)\n",
        "print('DCCA: tune acc={}, test acc={}'.format(valid_acc, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}